<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Joshua Boulton | Philosophy of Mind Researcher</title>
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body>
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light" id="mainNav">
            <div class="container px-4 px-lg-5">
                <a class="navbar-brand" href="index.html">Joshua Boulton</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto py-4 py-lg-0">
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="index.html">Home</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="about.html">About</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="research.html">Research Interests</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="contact.html">Contact</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Page Header-->
        <header class="masthead" style="background-image: url('assets/img/home-bg.jpg')">
            <div class="container position-relative px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <div class="post-heading">
                            <h1>How to Build a Mind From Scratch</h1>
                            <h2 class="subheading">The connection between life, intelligence, consciousness and thought.</h2>
                            <span class="meta">
                                [DRAFT] posted by
                                <a href="#!">Joshua Boulton</a>
                                on September 25, 2024 (updated: September 27, 2024)
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </header>
        <!-- Post Content-->
        <article class="mb-4">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <blockquote class="blockquote">To protect various unpublished works from plagiarism, I make a number of bold assertions in this post that I do not justify here. The aim of this post is to pique the interest of potential collaborators, especially a PhD supervisor.</blockquote>

                        <blockquote class="blockquote">Much of the earlier material in the post is largely in line with popular 'predictive processing', 'predictive coding' and 'active inference' models of intelligent brain function and perception. However, the minute details really matter when it comes to the philosophical application of these ideas that comprises the later material.</blockquote>

                        <h3 class="section-heading">Contents</h3>
                        <ul>
                            <li><a href="#environmental_tracking"><u>Environmental Tracking and Predictive Processing</u>
                            <li><a href="#planning"><u><i>How to Plan:</i> model the world and simulate future experiences</u></a></li>
                            <li><a href="#cat_breakfast"><u><i>An Illustration</i>: the cat's breakfast</a></u></li>
                            <li><a href="#phil_of_mind"><u><i>Lessons for Philosophy</i>: from the intentional stance to the redness of red</u></a></li>
                            <li>
                              <ul>
                                <li><u><a href="#quotation">Quotation in the Language of Thought</a></u></li>
                                <li><u><a href="#intentional_stance">Social Cognition, the Intentional Stance, and Phenomenal Simulation</a></u></li>
                                <li><u><a href="#SQT"><i>State Quotation Theory:</i> A Physicalist Theory of Phenomenal Consciousness</a></u></li>
                              </ul>  
                            </li>
                            <li><u><a href="#conclusion">Closing Thoughts</a></u></li>
                        </ul>

                        <h3 class="section-heading" id="environmental_tracking">Environmental Tracking and Predictive Processing</h3>

                        <p>It is almost tautologically the case that to do well in the game of evolution, it is always ideal for an organism to be in a physical state that, given the current environmental conditions, makes that organism likely to self-replicate.</p>

                        <p>Of course, no game is that simple. It is always ideal for a boxer to be in a physical state that, given their opponent's position, is likely to knock the opponent unconscious!</p>

                        <p>Realistic strategies involve compromise and sacrifice. What do these strategies look like in evolution?</p>

                        <p><i>Some</i> organisms follow the <b>bet-hedging</b> strategy. They are never in a particularly optimal state with respect to the current environment, but tend to exist in a (mostly) permanent state that does <i>well enough</i> in many environments. Eventually, this state will become deadly, but usually not before the organism has had many offspring.</p>

                        <p><i>Other</i> organisms (<i>highly intelligent</i> ones) instead follow some variant of the <b>environmental tracking</b> strategy. As discussed by Andy Clark in his book <i>The Experience Machine</i>, this strategy was, until recently, thought to work in a largely bottom-up manner. The organism is equipped with sensory organs that collect information about the environment, the organism models and represents the environment, and the organism trusts its representational model, responding to the world as if the model is accurate.</p>
                            
                        <p>We now have a plethora of reasons to think that the environmental tracking strategy works more like this:</p>

                        <img src="assets/img/mind_diagram.jpg" class="img-fluid" alt="Responsive image" style="padding-top: 40px; padding-bottom: 30px;">
                        <blockquote class="blockquote">This is, of course, an oversimplification. The full picture would be overwhelming for some readers to look at! I will build upon this as we go along.</blockquote>

                        <p>While environmental trackers <i>do</i> collect evidence in the form of sensory stimulation, their perceptions are <i>not</i> primarily representations constructed from that evidence in a bottom-up manner, but instead <i>top-down predictions</i> of sensory stimulation.</p>

                        <p>In the diagram above, this means their experiences of the world are more informed by the 'sensory prediction' box than by the 'sensory state' box.</p>

                        <h3 class="section-heading" id="planning"><i>How to Plan:</i> model the world and simulate future experiences</h3>

                        <p>The primary purpose of sensory stimulation is to update, and thereby improve the quality of, a <i>model of the world</i> that is used to predict one's own sensory states.</p>

                        <p>Most organisms sit somewhere on a spectrum between the two strategies (and perhaps some alternatives). Trees are the classic example of organisms leaning heavily into the former, while we humans lean heavily into the latter. Certainly, humans have <i>some</i> inherent durability, and trees have <i>some</i> capacity for adapting to the environment mid-lifecycle.</p>

                        <p><b>The extent to which we lean into the second strategy is what makes us highly 'intelligent'.</b> We model more of the world in more detail (including more levels of abstraction), <b>and do much more with that model</b>. As I aim to eventually show, <b> this is also what makes us think, believe, desire, feel, and experience subjectivity</b> to a greater extent than 'less intelligent' organisms.</p>

                        <p>The predictive processing loop, in my view, looks like this:</p>
                        <ol>
                            <li>The organism's <i>predictive</i> sensory machine enters some state G.</li>
                            <li>The organism's sensory organs react to the environment by inducing some state Q in the <i>receptive</i> sensory machine.</li>
                            <li>If the organism's <i>comparator</i> <b>doesn't</b> detect a mismatch between the two machine states, the prediction was accurate, which tells the organism something good about the world model.</li>
                            <li>If the comparator <b>does</b> detect a mismatch, information about that mismatch is used to update the world model to reduce the likelihood of mismatches in the future. It is also used for <i>backtracking</i>, discussed soon.</li>
                            <li><b>While</b> these comparisons are being made, several <i>simulations</i> are run by the organism. Each simulation has its own sensory state machine, which is initialised to the state of the receptive sensory state machine. What happens to each simulated sensory state is determined by 1) the current world model, and 2) the sequences of <i>action signals</i> it receives from a system akin to Daniel Kahneman's 'fast' and 'intuitive' <i>system 1</i>.</li>
                            <li>There are two reasons to run multiple simulations. One reason is to predict the consequences of multiple action signals. The other is to account for the effects of unpredictable influences that, according to the world model, aren't solely controlled by the organism (e.g., wind or the behaviour of other organisms).</li>
                            <li>Through a process that can be modeled with a graph search algorithm, an optimal sequence of sensory states is allowed to escape the simulation, becoming the predictive perception of the organism. The action signals expected to cause such states (which can be thought of as the steps of a <i>plan</i>) are also allowed to escape the simulation, entering the body's control systems.</li>
                            <li>In the case of a false prediction, an organism doesn't typically abandon its entire current plan, which might have been <i>mostly</i> accurate. It first tries <i>backtracking</i> through the graph constructed by the simulations to see if some of its plan can be salvaged - there might be another pathway from here to the goal state.</li>
                            <li>Return to step 1.</li>
                        </ol>
                        
                        <h3 class="section-heading" id="cat_breakfast"><i>An Illustration</i>: the cat's breakfast</h3>

                        <img src="assets/img/mind_diagram.jpg" class="img-fluid" alt="Responsive image" style="padding-top: 40px; padding-bottom: 30px;">
    
                        <blockquote class="blockquote">Here's that diagram again for reference.</blockquote>

                        <p>Our example organism is <i>Laika</i>, a fat, strong, fluffy, four-legged animal. Laika has ears, eyes, claws, sharp teeth, muscular limbs, vocal chords, and an almost insatiable appetite.</p>

                        <p>For Laika, the 'good life' is simple:</p>

                        <ul>
                            <li>Be nourished and hydrated.</li>
                            <li>Be warm, but not <i>too</i> warm.</li>
                            <li>Above all else: <b>have lots of kids.</b></li>
                        </ul>

                        <p>Laika's species has evolved to adapt to a wide variety of possible scenarios by behaving in ways that tend to induce sensory machine states that reliably indicate the fulfillment of these primal needs. <b>These are the fundamental <i>goal states</i></b>.</p>
                        
                        <p>The machine enters a specific sub-state when (and only when) the body's average temperature is too low. It enters another such state when (and only when) the stomach is full. Laika's genes drive him to behave in ways that bring about these machine states as often and for as long as possible.</p>

                        <p><b>These are complex goals. Many sub-goals follow from them, and will differ from one organism and one situation to the next.</b> </p>
                        
                        <p>From the womb of Laika's mother, it could not be determined whether Laika would live in a forest (in which satiation would depend on his ability to hunt native birds), or an inner-city flat (in which a large bipedal organism would somehow manage to deliver processed meat twice a day).</p>

                        <p>The bet-hedging strategy doesn't work for an organism like Laika. Laika, finding himself in one of many possible scenarios, must optimise for that one.</p>

                        <p><b><i>Fast-forward:</i></b> Laika's sensory state machine is in the state that reliably indicates a certain pattern of activation on his retinas, certain frequencies in his ears, a rumbling stomach, high levels of ghrelin, and low levels of leptin. <b>Laika is hungry, and there is a hairless ape within his field of view that has the power to open bags of biscuits.</b></p>

                        <p>A system akin to Daniel Kahneman's 'fast' and 'intuitive' <i>system 1</i> responds to Laika's sensory state by sending a handful of neural firing patterns, each a series of action signals, into a handful of brain regions ('threads') that each run a simulation of Laika's sensory state machine.</p>
                            
                        <p>Each thread's behaviour is (ideally) determined by:</p>
                        <ol>
                            <li>the world model,</li>
                            <li>the initial state (Laika's current sensory state)</li>
                            <li>the input action signals, and</li>
                            <li>variables that, according to the world model, may be altered unexpectedly (e.g. by other agents and/or objects in the world).</li>
                        </ol>

                        <img src="assets/img/planning_diagram.jpg" class="img-fluid" alt="Responsive image" style="padding-top: 40px; padding-bottom: 30px;">
                        <blockquote class="blockquote">The duplicate states on the left represent the fact that each thread has its own sensory state machine. The common states to which they connect represent the fact that the system uses all of these threads to construct a plan in a manner that could be modeled by a graph search algorithm.</blockquote>
                        <blockquote class="blockquote">State 5 is a goal state, state 2 isn't great, state 3 is decent, and state 4 is really bad. </blockquote>
                        <blockquote class="blockquote">The difference between state 2 -> state 4 and state 2 -> state 5 is beyond Laika's control, according to his current world model.</blockquote>
                        <blockquote class="blockquote">State 3 isn't as good as state 5, but the simulation also predicts it to be a low-risk potential sub-goal.</blockquote>

                        <p>Suppose one of these sequences causes many 'threads' of the simulation (sensory state machines, hooked up to the world model) to undergo sequences of states that end in a <b>bad state</b>. This sequence of action signals <b>must not</b> make it out of the simulation! According to the world model, they will cause the body to interact with the environment <i>terribly</i>.</p>

                        <p>Another sequence causes many threads of the simulation to undergo sequences of states that end in a goal state. A <i>few</i> of these threads don't end in a goal state, signalling that, according to the world model, certain possible events would prevent these actions from being useful. <b>Laika is not the only agent in the world, and even many non-agents are highly unpredictable. But that's the reality of life! Our plans <i>don't</i> always work - we still attempt them.</b> Many of these threads undergo very similar sequences of states and <b>the common simulated sensory states are likely to becomes temporary sub-goal states.</b></p>

                        <p>Something <i>like <b>this</b></i> sequence of action signals <b>should</b> make it out of the simulation, into the control centers of the body!</p>

                        <p>Now suppose one action signal, executed in the current sensory state, preceded some sub-goal state in many simulation threads. This signal is sent throughout Laika's body, and that sub-goal state is compared to the state that Laika's sensory state machine enters. The two states are, for the most part, similar. Laika is one step closer to the ultimate goal.</p>

                        <p>Next, an action signal that, in the <i>new</i> sensory state, reliably induced the <i>next</i> simulated sub-goal state is sent throughout Laika's body. This time, the state of Laika's sensory state machine does <i>not</i> match the expected sub-goal state at all. Something unexpected has happened. <b>If some simulaton threads can explain this fact, Laika should <i>backtrack</i>. Otherwise, he should update his world model.</b></p>

                        <p>Perhaps Laika tries backtracking, and the 'next best' action signal from the simulation <i>does</i> lead to a predictive match. Laika is one step closer to the ultimate goal. The last two 'observations' (made by the <i>comparator</i> in the diagram) may even tell Laika something about what is <i>right</i> in the world model and what is <i>wrong</i>.</p>

                        <p>Maybe Laika ends up at a deadend, failing to find the goal-state. Backtracking has failed. The world model will undergo significant updates, further simulations may be run, and a new sequence of sub-goals may be identified.</p>

                        <p>Hopefully, <b>Laika soon detects the state that reliably indicates satiation - a goal state.</b> The action signals that have won the competition of neural attention have caused his gaze to meet that of the mysterious bipedal food-giver, his vocal chords to emit an adorable <i>miaow</i>, and his limbs to drag him towards the bag of biscuits by the shelf. The bipedal organism has opened the bag and poured some of its contents into a bowl, eventually consumed by Laika, triggering hormonal responses and a desirable sensory state (for now!).</p>

                        <h3 class="section-heading" id="phil_of_mind">Lessons for Philosophy</h3>

                        <blockquote class="blockquote">I've seen a lot of these ideas in the sciences. What's this got to do with <i>philosophy</i>, and what do you have to add?</blockquote>

                        <p>I have reason to believe that the story told above can tell us a lot about qualia, physicalism, intentionality, self-consciousness, higher-order thought theory, strong AI, and metaphilosophy. Here, I will focus on one of these.</p>

                        <h4 class="section-heading" id="quotation">Quotation in the Language of Thought</h4>

                        <p><b>It is highly beneficial for some organisms to evolve a language of thought in which the states of the sensory machines are denoted <i>quotationally</i></b>. Our <i>phenomenal concepts</i> are quotations of these machine states. (For a very <i>similar</i> account of phenomenal concepts, see <a href="https://philarchive.org/rec/BALAAT">Balog, 2012</a>.). This minimizes the probability of making <i>bad updates</i> (i.e., misinformed ones) to a <i>very</i> important model.</p>

                        <p>Researchers interested in the quotational account of phenomenal concepts haven't explored the connection between predictive processing and mental quotation. This is one reason why many things have gone unnoticed about quotational concepts.</p>

                        <h4 class="section-heading" id="intentional_stance">Social Cognition, the Intentional Stance, and Phenomenal Simulation</h4>
                        
                        <p>Quotational concepts would help the organism engage in a highly efficient form of social cognition that involves using one's own sensory state machines to simulate those of other organisms.</p>

                        <p>Daniel Dennett famously distinguished a number of <i>stances</i> via which intelligent agents can predict the behaviour of objects.</p>

                        <p>In taking the <i><b>design stance</b></i>, an agent assumes that some object has a certain <i>purpose</i>, and predicts that it will behave the way it is <i>supposed</i> to.</p>
                        <p>In taking the <i><b>physical stance</b></i>, an agent assumes that some object has certain properties and that certain laws dictate how the properties of the object interact with those of others. The agent predicts that the properties of the object will obey those laws.</p>
                        <p>In taking the <i><b>intentional stance</b></i>, the agent assumes that the object, like it, <i>takes stances</i>. It has goals and beliefs. It wants those beliefs to be accurate, and to tell it that its goals are being acheived. To these ends, it predicts and modifies the behaviour of its environment.</p>

                        <p>The kinds of systems most effectively predicted by the physical and intentional stances are highly complex, and both stances require the employment of many resources. <b>Those most effectively predicted via the intentional stance are especially complicated</b>, containing billions of neural structures, themselves taking the intentional stance.</p>

                        <p><b>For <i>Laika</i>, it is generally most useful to take the intentional stance towards objects that are quite similar to him</b>. The social relationships between Laika and other members of his species matter a great deal for his evolutionary success. <b>The birds he consumes, in the grand scheme of things, are relatively similar to his species</b>, being far more closely related to him than trees are. <b>They too are modeling and simulating the world, vocalising, responding to patterns of retinal activation, and moving their limbs. They breathe the same air and model the same laws of physics, <i>to some extent</i>.</b> The mysterious bipedal food-giver, a fellow mammal, is even more similar to Laika.</p>

                        <p><b>Laika's species</b> <i>could</i> evolve expensive resources with which they can simulate birds and hairless apes, but they <i>already</i> <b>have some expensive resources that, with a little tweaking, could quite effectively allow Laika to take the intentional stance towards such organisms</b> - just as (and largely <i>because</i>) Laika's body has a lot in common with theirs, <b>the part of his body that simulates the world has a lot in common with the parts of their bodies that simulate the world</b>. With a few tweaks of the genome, Laika might be able to <b>simulate the simulations</b> of these other organisms in a manner that is much more <i>projective</i> than the simulation of other kinds of things in the world.</p>

                        <h4 class="section-heading" id="SQT"><i>State Quotation Theory:</i> A Physicalist Theory of Phenomenal Consciousness</h4>

                        <p>To keep an already long story as short as possible: <b>this kind of simulation, with the assistance of quotational concepts, would give rise to a host of surprising phenomena.</b> One notable one is what I call <i>essential quotation</i> (similar to John Perry's "essential indexicality"). As Douglas Campbell and I argue in our upcoming paper, it would also explain:</p>
                        <ul>
                            <li>The epistemic gap (famously explored by Frank Jackson's "Mary's Room" thought experiment, in his knowledge argument against physicalism),</li>
                            <li>the explanatory gap,</li>
                            <li>the hard problem of phenomenal consciousness,</li>
                            <li>the intuition of direct acquaintance with phenomenal properties,</li>
                            <li>the intuitive appeal of conceivability arguments against identity theory and physicalism,</li>
                            <li>the counterintuitiveness of a purely relational account of qualia,</li>
                            <li>the counterintuitiveness of the Frege-Schlick view (a powerful response to the inverted spectrum argument against functionalism), and</li>
                            <li>the inter-species phenomenal conceivability and knowledge gaps identified by Thomas Nagel.</li>
                        </ul>
                        
                        <p>Perry's indexical account of phenomenal concepts solves <i>some</i> of these problems, but is subject to a number of objections. The quotational account, we argue, solves <i>all</i> of these while avoiding many of Perry's problems.</p>

                        <p>Additionally, I argue that the broad picture sketched above helps explain fits <i>very</i> neatly with:</p>
                        <ul>
                            <li>A compelling solution to known problems with higher-order thought theories of consciousness (see <a href="https://philpapers.org/rec/COLQHT">Coleman, 2015</a> and <a href="https://philarchive.org/rec/PICAHM">Picciuto, 2011</a>),</li>
                            <li>Campbell's solution to the hard problem of observer consciousness (See <a href="https://repository.arizona.edu/handle/10150/195372"><i>A Theory of Consciousness</i></a> by Douglas Campbell),</li>
                            <li>Campbell's 'simbeard' solution to the evolution of altruism (yet unpublished).</li>
                        </ul>
                        <h3 class="section-heading" id="conclusion">Closing Thoughts</h3>
                        <p>Those familiar with many of the issues I've discussed will understand the magnitude of my claim. It has implications for research into blindsight, qualia, intentionality, emotion, mental illness, ethics, AI, and more.</p>
                        <p>I am looking for a fully-funded PhD program, during which I can explain many of my claims, put them on trial, and explore these issues further.</p>
                        <p>If you've made it this far, thank you. Learn about my background <a href="about.html">here</a> and get in touch <a href="contact.html">here</a>.</p>
                    </div>
                </div>
            </div>
        </article>
        <!-- Footer-->
        <footer class="border-top">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <ul class="list-inline text-center">
                            <li class="list-inline-item">
                                <a href="https://www.linkedin.com/in/jjrboulton/" target="_blank">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-linkedin-in fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li class="list-inline-item">
                                <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-facebook-f fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li class="list-inline-item">
                                <a href="https://github.com/joshuaboulton" target="_blank">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                        </ul>
                        <div class="small text-center text-muted fst-italic">Copyright &copy; Joshua Boulton 2024</div>
                    </div>
                </div>
            </div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
