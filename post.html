<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Joshua Boulton | Philosophy of Mind Researcher</title>
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body>
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light" id="mainNav">
            <div class="container px-4 px-lg-5">
                <a class="navbar-brand" href="index.html">Joshua Boulton</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto py-4 py-lg-0">
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="index.html">Home</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="about.html">About</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="research.html">Research Interests</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="contact.html">Contact</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Page Header-->
        <header class="masthead" style="background-image: url('assets/img/home-bg.jpg')">
            <div class="container position-relative px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <div class="post-heading">
                            <h1>How to Build a Mind From Scratch</h1>
                            <h2 class="subheading">The connection between life, intelligence, consciousness and thought.</h2>
                            <span class="meta">
                                [DRAFT] Posted by
                                <a href="#!">Joshua Boulton</a>
                                on September 25, 2024
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </header>
        <!-- Post Content-->
        <article class="mb-4">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <blockquote class="blockquote">To protect various unpublished works from plagiarism, I make a number of bold assertions in this post that I do not justify here. The aim of this post is to pique the interest of potential collaborators, especially a PhD supervisor.</blockquote>

                        <blockquote style="font-style: italic; color: #0085A1;">Please note that a great deal of this post assumes the reader has some background in philosophy of mind and/or cognitive science.</blockquote>

                        <h2 class="section-heading">Contents</h2>
                        <ul>
                            <li><a href="#a_diagram"><i>A Diagram</i></a></li>
                            <li><a href="#environmental_tracking"><i>Environmental Tracking and Predictive Processing</i></a></li>
                            <li><a href="#simulation"><i>The Mind's Eye</i>: simulating the world</a></li>
                            <li><a href="#cat_breakfast"><i>An Illustration</i>: the cat's breakfast</a></li>
                            <li><a href="#phil_of_mind"><i>Lessons for Philosophy</i>: social cognition, physicalism, and the redness of red</a></li>
                        </ul>

                        <p>To win the game of evolution, an organism should be in bodily states that tend to increase its evolutionary fitness (in the current environment) as often as possible.</p>

                        <p><i>Some</i> organisms follow the <b>bet-hedging</b> strategy. They are never in a particularly optimal state with respect to the current environment, but tend to exist in a (mostly) permanent state that does <i>well enough</i> in many environments. Eventually, this state will become deadly, but usually not before the organism has had many offspring.</p>

                        <p><i>Other</i> organisms (<i>highly intelligent</i> ones) instead follow a variant of the <b>environmental tracking</b> strategy. This variant looks something like this (soon to be unpacked):</p>

                        <img src="assets/img/mind_diagram.jpg" class="img-fluid" alt="Responsive image" style="padding-top: 40px; padding-bottom: 30px;">

                        <p>Most organisms sit somewhere on a spectrum between the two strategies (and perhaps some alternatives). Trees are the classic example of organisms leaning heavily into the former, while humans lean heavily into the latter. Certainly, humans have <i>some</i> inherent durability, and trees have <i>some</i> capacity for adapting to the environment mid-lifecycle.</p>

                        <p><b>The extent to which we lean into the second strategy is what makes us highly 'intelligent'.</b> We model much more of the world in more detail (including more levels of abstraction), and do much more with that model. As I aim to eventually show, <b> this is also what makes us think, believe, desire, feel, and experience subjectivity</b> to a greater extent than 'less intelligent' organisms.</p>

                        <h2 class="section-heading">What's happening in the diagram?</h2>

                        <p>The organism has a <i>sensory state machine</i> that reliably tracks what is happening in the sensory organs, interoceptive monitoring systems, and so on. The machine's states function as high-quality evidence regarding what is happening in one small, important part of the world: itself. (<b>As I later explain, the organism has more than just <i>evidence</i>: it has <i>direct acquaintance</i> with part of the physical world.</b>)</p>

                        <p>The organism is inherently capable of altering the world in various ways that would induce <b>goal states, which are states of the aforementioned machine that reliably track fitness-enhancing scenarios.</b> By sending the appropriate <i>action signals</i> to its control systems, it can move its limbs, make sounds that other organism react to, light fires, and swallow nutritious fruits.</p>
                        
                        <h3 class="section-heading">How does it learn to promote the right action signals in the right situations? The world is complicated!</h3>

                        <p><b>It uses the evidence acquired from the sensory state machine to develop an accurate <i>world model</i></b>: a complex representation of the world containing information about: </p>
                        <ul>
                            <li>How the body is affected by both external objects and its own internally-generated behaviour (i.e. the consequences of particular action signals).</li>
                            <li>What has happened in the past.</li>
                            <li>What will likely happen in the future.</li>
                            <li>Why past events happened - part of <b>what makes an organism confident in its world model is its ability to simulate its memories </b> (including the emergence of the current sensory state) using that model.</li>
                        </ul>

                        <h2 class="section-heading">What's the point of the world model?</h2>

                        <p><b>The world model is used to run simulations of the world in which the organism can plan its behavioural response to any given sensory state.</b></p>

                        <p>Simulations are informed by the current sensory state, the current world model, and heuristics that select candidate action signals to send to the <i>simulated body</i> (an entity in the simulation that represents the part of the world that is <i>always <b>here</b></i>, acting upon the simulated consequences of action signals!).</p>

                        <p><b>Each simulation has its own sensory state machine</b>, and the states of that machine are the predicted consequences of the input action signals in some possible world.</b></p>
                        
                        <p>Assuming the model is highly accurate, the consequences of certain action signals in the simulation can be a useful guide to consequences of such signals went sent to the body's control systems.</p>
                        
                        <p>If a sequence of action signals (in conjunction with a sequence of predicted external events) gives rise to warmth, safety, sexual success, satiety, social stability, and bodily integrity in an accurate simulation, it is beneficial for the organism to start sending that sequence of action signals to the control systems of its nonsimulated, <i>actual</i> body, and to prepare action signals to be triggered by certain anticipated possible events.</p>

                        <h2 class="section-heading">What if the simulation is wrong?</h2>

                        <p>Sometimes these action signals will <i>not</i> have the consequences observed in the simulation. <b>This can be detected using the evidence from that state machine, which can be used to update the world model,</b> decreasing the chance of inaccurate predictions in the future.</p>

                        <h2 class="section-heading">Let's consider an example.</h2>

                        <img src="assets/img/mind_diagram.jpg" class="img-fluid" alt="Responsive image" style="padding-top: 40px; padding-bottom: 30px;">
    
                        <p>Here's that diagram again for reference.</p>

                        <p>Our first example organism is <i>Laika</i>, a fat, strong, fluffy, four-legged animal. Laika has ears, eyes, claws, sharp teeth, muscular limbs, vocal chords, and an almost insatiable appetite.</p>

                        <p>For Laika, the 'good life' is simple:</p>

                        <ul>
                            <li>Be nourished and hydrated.</li>
                            <li>Be warm, but not <i>too</i> warm.</li>
                            <li>Above all else: <b>have lots of kids.</b></li>
                        </ul>

                        <p>Laika's species has evolved to adapt to a wide variety of possible scenarios by behaving in ways that tend to induce sensory machine states that reliably indicate the fulfillment of these primal needs. <b>These are the fundamental <i>goal states</i></b>.</p>
                        
                        <p>The machine enters a specific sub-state when (and only when) the body's average temperature is too low. It enters another such state when (and only when) the stomach is full. Laika's genes drive him to behave in ways that bring about these machine states as often and for as long as possible.</p>

                        <p><b>These are complex goals. Many sub-goals follow from them, and will differ from one organism and one situation to the next.</b> </p>
                        
                        <p>From the womb of Laika's mother, it could not be determined whether Laika would live in a forest (in which satiation would depend on his ability to hunt native birds), or an inner-city flat (in which a large bipedal organism would somehow manage to deliver processed meat twice a day).</p>

                        <p>The bet-hedging strategy doesn't work for an organism like Laika. Laika, finding himself in one of many possible scenarios, must optimise for that one.</p>

                        <p><b><i>Fast-forward:</i></b> Laika's sensory state machine is in the state that reliably indicates a certain pattern of activation on his retinas, certain frequencies in his ears, a rumbling stomach, high levels of ghrelin, and low levels of leptin. <b>Laika is hungry, and there is a hairless ape within his field of view that has the power to open bags of biscuits.</b></p>

                        <p>A system akin to Daniel Kahneman's 'fast' and 'intuitive' <i>system 1</i> responds to Laika's sensory state by sending a handful of neural firing patterns, each a series of action signals, into a handful of brain regions ('threads') that each run a simulation of Laika's sensory state machine.</p>
                            
                        <p>Each thread's behaviour is (ideally) determined by:</p>
                        <ol>
                            <li>the world model,</li>
                            <li>the initial state (Laika's current sensory state)</li>
                            <li>the input action signals, and</li>
                            <li>variables that, according to the world model, may be altered unexpectedly (e.g. by other agents and/or objects in the world).</li>
                        </ol>

                        <img src="assets/img/planning_diagram.jpg" class="img-fluid" alt="Responsive image" style="padding-top: 40px; padding-bottom: 30px;">
                        <blockquote class="blockquote">The duplicate states on the left represent the fact that each thread has its own sensory state machine. The common states to which they connect represent the fact that the system uses all of these threads to construct a plan in a manner that could be modeled by a graph search algorithm.</blockquote>
                        <blockquote class="blockquote">State 5 is a goal state, state 2 isn't great, state 3 is decent, and state 4 is really bad. </blockquote>
                        <blockquote class="blockquote">The difference between state 2 -> state 4 and state 2 -> state 5 is beyond Laika's control, according to his current world model.</blockquote>
                        <blockquote class="blockquote">State 3 isn't as good as state 5, but the simulation also predicts it to be a low-risk potential sub-goal.</blockquote>

                        <p>One of these sequences causes many 'threads' of the simulation (sensory state machines, hooked up to the world model) to undergo sequences of states that end in a <b>bad state</b>. This sequence <b>must not</b> make it out of the simulation!</p>

                        <p>Another sequence causes many threads of the simulation to undergo sequences of states that end in a goal state. A few of these threads don't end in a goal state. Many of these threads undergo very similar sequences of states. <b>The common simulated sensory states are likely to becomes temporary sub-goal states.</b></p>

                        <p>Something <i>like <b>this</b></i> sequence of action signals <b>should</b> make it out of the simulation, into the control centers of the body!</p>

                        <p>Suppose one action signal, executed in the current sensory state, preceded some sub-goal state in many simulation threads. This signal is sent throughout Laika's body, and that sub-goal state is compared to the state that Laika's sensory state machine enters. The two states are, for the most part, similar. Laika is one step closer to the ultimate goal.</p>

                        <p>Next, an action signal that, in the <i>new</i> sensory state, reliably induced the <i>next</i> simulated sub-goal state is sent throughout Laika's body. This time, the state of Laika's sensory state machine does <i>not</i> match the expected sub-goal state at all. Something unexpected has happened, or something is very wrong with the model.</p>

                        <p>Perhaps the next best action signal from the simulation is tried, and this <i>does</i> lead to a predictive match. Laika is one step closer to the ultimate goal, and the last two 'observations' (made by the <i>comparator</i> in the diagram) may already tell it something about what is <i>right</i> in the world model and what is <i>wrong</i>.</p>

                        <p>Maybe Laika ends up at a deadend, failing to find the goal-state. The world model will undergo significant updates, further simulations may be run, and a new sequence of sub-goals may be identified.</p>

                        <p>Maybe Laika soon detects the state that reliably indicates satiation. The action signals that have won the competition of neural attention have caused his gaze to meet that of the mysterious bipedal food-giver, his vocal chords to emit an adorable <i>miaow</i>, and his limbs to drag him towards the bag of biscuits by the shelf. The bipedal organism has opened the bag and poured some of its contents into a bowl, eventually consumed by Laika, triggering hormonal responses and a desirable sensory state (for now!).</p>

                        <h3 class="section-heading">I've seen a lot of these ideas in the sciences. What's this got to do with <i>philosophy</i>, and what do you have to add?</h3>

                        <p>I have reason to believe that the story told above can tell us a lot about qualia, physicalism, intentionality, self-consciousness, higher-order thought theory, strong AI, and metaphilosophy. Here, I will focus on one of these.</p>

                        <p><b>It is highly beneficial for some organisms to evolve a language of thought in which the states of the sensory machines are denoted <i>quotationally</i></b>. Our <i>phenomenal concepts</i> are quotations of these machine states. (For a very <i>similar</i> account of phenomenal concepts, see <a href="https://philarchive.org/rec/BALAAT">Balog, 2012</a>.). This minimizes the probability of making <i>bad updates</i> (i.e., misinformed ones) to a <i>very</i> important model.</p>

                        <p>Researchers interested in the quotational account of phenomenal concepts haven't explored the connection between predictive processing and mental quotation. This is one reason why many things have gone unnoticed about quotational concepts.</p>

                        <h3 class="section-heading">Social Cognition and Simulation</h3>
                        
                        <p>Quotational concepts would help the organism engage in a highly efficient form of social cognition that involves using one's own sensory state machines to simulate those of other organisms.</p>

                        <p>Daniel Dennett famously distinguished a number of <i>stances</i> via which intelligent agents can predict the behaviour of objects.</p>

                        <p>In taking the <i><b>design stance</b></i>, an agent assumes that some object has a certain <i>purpose</i>, and predicts that it will behave the way it is <i>supposed</i> to.</p>
                        <p>In taking the <i><b>physical stance</b></i>, an agent assumes that some object has certain properties and that certain laws dictate how the properties of the object interact with those of others. The agent predicts that the properties of the object will obey those laws.</p>
                        <p>In taking the <i><b>intentional stance</b></i>, the agent assumes that the object, like it, <i>takes stances</i>. It has goals and beliefs. It wants those beliefs to be accurate, and to tell it that its goals are being acheived. To these ends, it predicts and modifies the behaviour of its environment.</p>

                        <p>The kinds of systems most effectively predicted by the physical and intentional stances are highly complex, and both stances require the employment of many resources. <b>Those most effectively predicted via the intentional stance are especially complicated</b>, containing billions of neural structures, themselves taking the intentional stance.</p>

                        <p><b>For <i>Laika</i>, it is generally most useful to take the intentional stance towards objects that are quite similar to him</b>. The social relationships between Laika and other members of his species matter a great deal for his evolutionary success. <b>The birds he consumes, in the grand scheme of things, are relatively similar to his species</b>, being far more closely related to him than trees are. <b>They too are modeling and simulating the world, vocalising, responding to patterns of retinal activation, and moving their limbs. They breathe the same air and model the same laws of physics, <i>to some extent</i>.</b> The mysterious bipedal food-giver, a fellow mammal, is even more similar to Laika.</p>

                        <p><b>Laika's species</b> <i>could</i> evolve expensive resources with which they can simulate birds and hairless apes, but they <i>already</i> <b>have some expensive resources that, with a little tweaking, could quite effectively allow Laika to take the intentional stance towards such organisms</b> - just as (and largely <i>because</i>) Laika's body has a lot in common with theirs, <b>the part of his body that simulates the world has a lot in common with the parts of their bodies that simulate the world</b>. With a few tweaks of the genome, Laika might be able to <b>simulate the simulations</b> of these other organisms in a manner that is much more <i>projective</i> than the simulation of other kinds of things in the world.</p>

                        <h3 class="section-heading">A Physicalist Theory of Phenomenal Consciousness</h3>

                        <p>To keep an already long story as short as possible: <b>this kind of simulation, with the assistance of quotational concepts, would give rise to a host of surprising phenomena.</b> One notable one is what I call <i>essential quotation</i> (similar to John Perry's "essential indexicality"). As Douglas Campbell and I argue in our upcoming paper, it would also explain:</p>
                        <ul>
                            <li>The epistemic gap (famously explored by Frank Jackson's "Mary's Room" thought experiment, in his knowledge argument against physicalism),</li>
                            <li>the explanatory gap,</li>
                            <li>the hard problem of phenomenal consciousness,</li>
                            <li>the intuition of direct acquaintance with phenomenal properties,</li>
                            <li>the intuitive appeal of conceivability arguments against identity theory and physicalism,</li>
                            <li>the counterintuitiveness of a purely relational account of qualia,</li>
                            <li>the counterintuitiveness of the Frege-Schlick view (a powerful response to the inverted spectrum argument against functionalism),</li>
                            <li>the inter-species phenomenal conceivability and knowledge gaps identified by Thomas Nagel,</li>
                            <li>and many other phenomena.</li>
                        </ul>
                        
                        <p>Perry's indexical account of phenomenal concepts solves <i>some</i> of these problems, but is subject to a number of objections. The quotational account, we argue, solves <i>all</i> of these while avoiding many of Perry's problems.</p>

                        <p>Additionally, the broad picture sketched above fits <i>very</i> neatly with:</p>
                        <ul>
                            <li>A compelling solution to known problems with higher-order thought theories of consciousness (see <a href="https://philpapers.org/rec/COLQHT">Coleman, 2015</a> and <a href="https://philarchive.org/rec/PICAHM">Picciuto, 2011</a>),</li>
                            <li>Campbell's solution to the hard problem of observer consciousness (See <a href="https://repository.arizona.edu/handle/10150/195372"><i>A Theory of Consciousness</i></a> by Douglas Campbell),</li>
                            <li>Campbell's 'simbeard' solution to the evolution of altruism (yet unpublished).</li>
                        </ul>
                        <h2 class="section-heading">Closing Thoughts</h2>
                        <p>Those familiar with many of the issues I've discussed will understand the magnitude of my claim. It has implications for research into blindsight, qualia, intentionality, emotion, mental illness, ethics, AI, and more.</p>
                        <p>I am looking for a fully-funded PhD program, during which I can explain many of my claims, put them on trial, and explore these issues further.</p>
                        <p>If you've made it this far, thank you. Learn about my background <a href="about.html">here</a> and get in touch <a href="contact.html">here</a>.</p>
                    </div>
                </div>
            </div>
        </article>
        <!-- Footer-->
        <footer class="border-top">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <ul class="list-inline text-center">
                            <li class="list-inline-item">
                                <a href="https://www.linkedin.com/in/jjrboulton/" target="_blank">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-linkedin-in fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li class="list-inline-item">
                                <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-facebook-f fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li class="list-inline-item">
                                <a href="https://github.com/joshuaboulton" target="_blank">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                        </ul>
                        <div class="small text-center text-muted fst-italic">Copyright &copy; Joshua Boulton 2024</div>
                    </div>
                </div>
            </div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
